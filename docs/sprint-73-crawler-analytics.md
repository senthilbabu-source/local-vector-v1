# Sprint 73 â€” AI Crawler Analytics (Wire `crawler_hits` in Middleware)

> **Claude Code Prompt â€” First-Pass Ready**
> Paste this entire prompt into VS Code Claude Code (Cmd+L).
> Upload alongside: `AI_RULES.md`, `CLAUDE.md`, `DEVLOG.md`, `prod_schema.sql`, `golden-tenant.ts`, `database.types.ts`

---

## ğŸ¯ Objective

Wire the **existing but empty `crawler_hits` table** to the proxy middleware so that every AI bot visit to a Magic Menu page is detected and logged. Then build a dashboard page that shows per-bot visit counts, blind spot detection, and fix recommendations. This is the "MONITOR" stage of the Intelligence Flywheel â€” the BIG WOW that no local restaurant AEO tool offers.

**Why this matters:** "GPTBot visited your menu page 47 times this month. PerplexityBot: 0 times â€” that's why Perplexity doesn't know your menu." Restaurant owners can literally SEE which AI engines know about them. Profound charges $399/month for Agent Analytics. LocalVector offers it at every tier because we host the Magic Menu pages.

---

## ğŸ“‹ Pre-Flight Checklist â€” READ THESE FILES FIRST

Before writing ANY code, read these files in order:

```
Read docs/AI_RULES.md                          â€” All 39 engineering rules
Read CLAUDE.md                                 â€” Project context + architecture
Read supabase/prod_schema.sql                  â€” Canonical schema (Â§1) â€” find crawler_hits table
Read lib/supabase/database.types.ts            â€” TypeScript DB types (Â§38) â€” find crawler_hits types
Read src/__fixtures__/golden-tenant.ts          â€” Golden Tenant fixtures (Â§4)
Read proxy.ts                                  â€” Middleware (AI_RULES Â§6, Â§37.3) â€” THIS IS WHERE BOT DETECTION GOES
Read middleware.ts                              â€” Re-export shim (DO NOT edit this file)
Read app/m/[slug]/page.tsx                     â€” Magic Menu public page (menu.* subdomain)
Read lib/plan-enforcer.ts                      â€” Plan gating functions (Â§5)
Read components/layout/Sidebar.tsx             â€” NAV_ITEMS array for new nav entry
Read lib/supabase/server.ts                    â€” createClient() vs createServiceRoleClient() (Â§18)
```

---

## ğŸ—ï¸ Architecture â€” What to Build

### Component 1: Bot Detection Utility â€” `lib/crawler/bot-detector.ts`

**Pure function.** Takes a `User-Agent` string, returns the matched bot or `null`. No I/O, no side effects.

```typescript
/**
 * AI bot user-agent detection â€” pure function.
 * Returns the canonical bot_type string if the UA matches a known AI crawler, else null.
 */

export interface DetectedBot {
  /** Canonical identifier stored in crawler_hits.bot_type */
  botType: string;
  /** Human-readable label for the dashboard */
  label: string;
  /** Which AI engine this bot feeds */
  engine: string;
  /** Description of what this bot does */
  description: string;
}

/**
 * Known AI bot user-agents.
 * Update this registry when new AI crawlers emerge.
 * Each entry: [UA substring to match (case-insensitive), DetectedBot info]
 */
export const AI_BOT_REGISTRY: readonly [string, DetectedBot][] = [
  ['GPTBot',            { botType: 'gptbot',            label: 'GPTBot',          engine: 'ChatGPT',             description: 'OpenAI training crawler' }],
  ['OAI-SearchBot',     { botType: 'oai-searchbot',     label: 'OAI-SearchBot',   engine: 'ChatGPT Search',      description: 'ChatGPT live search' }],
  ['ChatGPT-User',      { botType: 'chatgpt-user',      label: 'ChatGPT-User',    engine: 'ChatGPT',             description: 'ChatGPT browsing mode' }],
  ['ClaudeBot',         { botType: 'claudebot',         label: 'ClaudeBot',       engine: 'Claude',              description: 'Anthropic training crawler' }],
  ['Google-Extended',   { botType: 'google-extended',   label: 'Google-Extended', engine: 'Gemini',              description: 'Gemini AI training' }],
  ['PerplexityBot',     { botType: 'perplexitybot',     label: 'PerplexityBot',   engine: 'Perplexity',          description: 'Perplexity search crawler' }],
  ['meta-externalagent',{ botType: 'meta-external',     label: 'Meta-External',   engine: 'Meta AI',             description: 'Meta AI training crawler' }],
  ['Bytespider',        { botType: 'bytespider',        label: 'Bytespider',      engine: 'TikTok/ByteDance',    description: 'ByteDance AI crawler' }],
  ['Amazonbot',         { botType: 'amazonbot',         label: 'Amazonbot',       engine: 'Amazon AI',           description: 'Amazon AI crawler' }],
  ['Applebot-Extended', { botType: 'applebot-extended', label: 'Applebot',        engine: 'Apple Intelligence',  description: 'Apple Siri/Intelligence' }],
] as const;

/**
 * Pure function â€” detects if a User-Agent string belongs to a known AI bot.
 * Returns DetectedBot if matched, null otherwise.
 * Matching is case-insensitive substring match.
 */
export function detectAIBot(userAgent: string | null | undefined): DetectedBot | null { ... }

/**
 * Returns the full registry for dashboard display.
 * Used to show ALL possible bots (including ones with 0 visits).
 */
export function getAllTrackedBots(): DetectedBot[] { ... }
```

**Matching rules:**
- Case-insensitive substring match: `userAgent.toLowerCase().includes(pattern.toLowerCase())`
- First match wins (order matters â€” `GPTBot` must match before a hypothetical `GPTBot-Extended`)
- `null`/`undefined`/empty UA â†’ return `null`

---

### Component 2: Middleware Bot Logging â€” Edit `proxy.ts`

**CRITICAL: Edit `proxy.ts` (AI_RULES Â§6, Â§37.3). NEVER edit `middleware.ts`.**

Add bot detection to the `menu.*` subdomain handler. When a request to a Magic Menu page is detected as an AI bot:

1. Detect the bot using `detectAIBot(request.headers.get('user-agent'))`
2. If bot detected, fire-and-forget a logging call (non-blocking)
3. Continue serving the page normally â€” bot logging MUST NOT slow down page delivery

**Architecture decision â€” fire-and-forget via `fetch()` to an internal API route:**

Middleware runs at the Edge. It cannot use `createServiceRoleClient()` (that requires Node.js runtime). Instead:

1. Middleware detects the bot
2. Middleware fires a `fetch()` to `POST /api/internal/crawler-log` (fire-and-forget, no `await`)
3. The internal API route uses `createServiceRoleClient()` to INSERT into `crawler_hits`
4. The middleware continues with the rewrite to `/m/` path

**Why not log directly in middleware?**
- Edge middleware cannot use `@supabase/supabase-js` (it requires Node.js `crypto` and `http` modules)
- `waitUntil()` is available on Vercel Edge but the Supabase SDK won't work there
- A fire-and-forget fetch to an internal route is the cleanest pattern

**Pattern in proxy.ts:**

```typescript
import { detectAIBot } from '@/lib/crawler/bot-detector';

// Inside the menu.* subdomain handler, BEFORE the rewrite:
const bot = detectAIBot(request.headers.get('user-agent'));
if (bot) {
  // Fire-and-forget â€” DO NOT await. Bot logging never blocks page delivery.
  const logUrl = new URL('/api/internal/crawler-log', request.url);
  fetch(logUrl.toString(), {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      botType: bot.botType,
      userAgent: request.headers.get('user-agent'),
      path: pathname,           // e.g. "/charcoal-n-chill"
      slug: pathname.slice(1),  // e.g. "charcoal-n-chill"
    }),
  }).catch(() => {}); // Absorbed â€” AI_RULES Â§17
}
// Continue with existing rewrite logic
```

**IMPORTANT:** The `fetch()` is intentionally NOT awaited. Use `.catch(() => {})` to absorb errors silently (AI_RULES Â§17 â€” side-effect resilience). A failed log MUST NEVER abort the page serve.

---

### Component 3: Internal Crawler Log Route â€” `app/api/internal/crawler-log/route.ts`

**NOT a cron route.** This is an internal endpoint called by the middleware. It uses `createServiceRoleClient()` because `crawler_hits` has `service_role_insert` policy only â€” no authenticated INSERT grants.

```typescript
import { NextResponse } from 'next/server';
import { createServiceRoleClient } from '@/lib/supabase/server';

/**
 * POST /api/internal/crawler-log
 * Internal endpoint â€” called by proxy.ts middleware when an AI bot is detected.
 * NOT public-facing â€” secured by checking origin or a shared secret.
 */
export async function POST(request: Request) {
  // â”€â”€ Auth: verify internal origin â”€â”€
  // Option A: Check a shared secret header
  const internalSecret = request.headers.get('x-internal-secret');
  if (internalSecret !== process.env.CRON_SECRET) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  const body = await request.json();
  const { botType, userAgent, slug } = body;

  if (!botType || !slug) {
    return NextResponse.json({ error: 'Missing required fields' }, { status: 400 });
  }

  const supabase = createServiceRoleClient();

  // â”€â”€ Look up the magic_menu by public_slug â”€â”€
  const { data: menu } = await supabase
    .from('magic_menus')
    .select('id, org_id')
    .eq('public_slug', slug)
    .eq('is_published', true)
    .maybeSingle();

  if (!menu) {
    // No published menu for this slug â€” skip logging
    return NextResponse.json({ ok: true, logged: false });
  }

  // â”€â”€ INSERT into crawler_hits â”€â”€
  const { error } = await supabase.from('crawler_hits').insert({
    org_id: menu.org_id,
    menu_id: menu.id,
    bot_type: botType,
    user_agent: userAgent ?? null,
  });

  if (error) {
    console.error('[crawler-log] INSERT failed:', error);
    return NextResponse.json({ ok: false, error: error.message }, { status: 500 });
  }

  return NextResponse.json({ ok: true, logged: true });
}
```

**Update proxy.ts fire-and-forget to include the internal secret:**

```typescript
fetch(logUrl.toString(), {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'x-internal-secret': process.env.CRON_SECRET ?? '',
  },
  body: JSON.stringify({ botType: bot.botType, userAgent: ua, slug }),
}).catch(() => {});
```

---

### Component 4: Data Fetcher â€” `lib/data/crawler-analytics.ts`

Fetches aggregated crawler data for the dashboard. Same service-injection pattern as `lib/data/dashboard.ts` (Sprint 64).

```typescript
import type { SupabaseClient } from '@supabase/supabase-js';
import type { Database } from '@/lib/supabase/database.types';
import { getAllTrackedBots, type DetectedBot } from '@/lib/crawler/bot-detector';

export interface CrawlerSummary {
  /** Per-bot visit count (last 30 days) */
  bots: BotActivity[];
  /** Total visits across all bots (last 30 days) */
  totalVisits: number;
  /** Bots with 0 visits â€” "blind spots" */
  blindSpots: BlindSpot[];
  /** Total number of blind spots */
  blindSpotCount: number;
}

export interface BotActivity {
  botType: string;
  label: string;
  engine: string;
  description: string;
  /** Visit count in the last 30 days */
  visitCount: number;
  /** Most recent visit timestamp, or null if never visited */
  lastVisitAt: string | null;
  /** Status derived from visitCount */
  status: 'active' | 'low' | 'blind_spot';
}

export interface BlindSpot {
  botType: string;
  label: string;
  engine: string;
  /** Human-readable fix recommendation */
  fixRecommendation: string;
}

/**
 * Fetches crawler analytics for an org.
 * Queries crawler_hits for the last 30 days, aggregates by bot_type.
 * Cross-references with AI_BOT_REGISTRY to detect blind spots.
 */
export async function fetchCrawlerAnalytics(
  supabase: SupabaseClient<Database>,
  orgId: string
): Promise<CrawlerSummary> { ... }
```

**Query pattern:**

```sql
SELECT bot_type,
       COUNT(*) as visit_count,
       MAX(crawled_at) as last_visit_at
FROM crawler_hits
WHERE org_id = :orgId
  AND crawled_at >= NOW() - INTERVAL '30 days'
GROUP BY bot_type
```

In Supabase JS, this can be done via an RPC (if you create a helper function) or by fetching raw rows and aggregating in TypeScript. **Prefer the TypeScript aggregation approach** â€” no new SQL function or migration needed:

```typescript
const thirtyDaysAgo = new Date();
thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);

const { data: hits } = await supabase
  .from('crawler_hits')
  .select('bot_type, crawled_at')
  .eq('org_id', orgId)
  .gte('crawled_at', thirtyDaysAgo.toISOString());
```

Then aggregate in TypeScript:
1. Group by `bot_type` â†’ count + max `crawled_at`
2. Cross-reference with `getAllTrackedBots()` â€” any bot in the registry with 0 hits is a blind spot
3. Status thresholds: `>= 5` visits = `active`, `1â€“4` visits = `low`, `0` visits = `blind_spot`

**Blind spot fix recommendations (static per engine):**

```typescript
const BLIND_SPOT_FIXES: Record<string, string> = {
  'gptbot':            'Ensure your robots.txt allows GPTBot. Submit your menu URL to ChatGPT via chat.',
  'oai-searchbot':     'OAI-SearchBot follows GPTBot rules. Allow GPTBot in robots.txt to enable ChatGPT Search.',
  'chatgpt-user':      'ChatGPT browsing requires GPTBot access. Check robots.txt and ensure your site loads without JavaScript.',
  'claudebot':         'Allow ClaudeBot in robots.txt. Claude indexes pages linked from structured data and sitemaps.',
  'google-extended':   'Google-Extended is controlled in Google Search Console. Ensure it\'s not blocked in robots.txt.',
  'perplexitybot':     'Allow PerplexityBot in robots.txt. Submit your URL to Perplexity via their web interface.',
  'meta-external':     'Allow meta-externalagent in robots.txt. Ensure your Facebook/Instagram business profiles link to your website.',
  'bytespider':        'Bytespider crawls pages linked from TikTok. Ensure your TikTok profile links to your menu page.',
  'amazonbot':         'Allow Amazonbot in robots.txt. Ensure your business is listed on Amazon/Alexa.',
  'applebot-extended': 'Allow Applebot-Extended in robots.txt. Register your business on Apple Maps Connect.',
};
```

---

### Component 5: Dashboard Page â€” `app/dashboard/crawler-analytics/page.tsx`

**New dashboard page â€” Server Component.**

**Sidebar entry:** Add to `NAV_ITEMS` in `components/layout/Sidebar.tsx`:
- Label: `"Bot Activity"`
- Icon: `Bot` from `lucide-react`
- Href: `/dashboard/crawler-analytics`
- Position: after "Page Audits" and before "AI Assistant"

**Page layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Bot Activity                                     Last 30 daysâ”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Total    â”‚  â”‚  Active   â”‚  â”‚  Blind   â”‚                       â”‚
â”‚  â”‚  127      â”‚  â”‚  5 bots   â”‚  â”‚  Spots   â”‚                       â”‚
â”‚  â”‚  visits   â”‚  â”‚           â”‚  â”‚  3       â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                   â”‚
â”‚  Bot Activity                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  GPTBot           47 visits   âœ… ChatGPT knows you               â”‚
â”‚  Google-Extended   89 visits   âœ… Gemini training                 â”‚
â”‚  OAI-SearchBot     23 visits   âœ… ChatGPT Search active          â”‚
â”‚  ClaudeBot         12 visits   âœ… Claude indexing                 â”‚
â”‚  Applebot           3 visits   âš ï¸ Low activity                   â”‚
â”‚  PerplexityBot      0 visits   âŒ Blind spot                     â”‚
â”‚  Meta-External      0 visits   âŒ Blind spot                     â”‚
â”‚  Bytespider         0 visits   âŒ Blind spot                     â”‚
â”‚                                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  ğŸ” 3 Blind Spots Detected                                       â”‚
â”‚                                                                   â”‚
â”‚  âŒ PerplexityBot â€” Perplexity is guessing about you             â”‚
â”‚     â†’ Allow PerplexityBot in robots.txt. Submit your URL to      â”‚
â”‚       Perplexity via their web interface.                         â”‚
â”‚                                                                   â”‚
â”‚  âŒ Meta-External â€” Meta AI can't see your content               â”‚
â”‚     â†’ Allow meta-externalagent in robots.txt. Ensure your        â”‚
â”‚       Facebook/Instagram business profiles link to your website.  â”‚
â”‚                                                                   â”‚
â”‚  âŒ Bytespider â€” TikTok AI has no data about you                 â”‚
â”‚     â†’ Bytespider crawls pages linked from TikTok. Ensure your    â”‚
â”‚       TikTok profile links to your menu page.                     â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Null state (no crawler hits at all â€” brand new tenant):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Bot Activity                                                 â”‚
â”‚                                                                   â”‚
â”‚  No bot visits recorded yet. AI crawlers will be automatically   â”‚
â”‚  detected when they visit your published Magic Menu page.         â”‚
â”‚                                                                   â”‚
â”‚  â†’ Make sure your Magic Menu is published to start tracking.      â”‚
â”‚  â†’ Check back in a few days â€” AI bots crawl new pages weekly.    â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation rules:**
- Server Component â€” data fetched server-side, no `'use client'` (AI_RULES Â§6).
- Auth via `getSafeAuthContext()` (AI_RULES Â§3).
- Belt-and-suspenders: `org_id` filter on all queries (AI_RULES Â§18).
- Status icons: `âœ…` active, `âš ï¸` low, `âŒ` blind spot â€” use literal Tailwind color classes (AI_RULES Â§12).
- Sort bots by visit count descending (most active first).
- **No plan gating.** Bot Activity is available to ALL tiers (even trial). It's the wow that sells upgrades.

**Error boundary:** Create `app/dashboard/crawler-analytics/error.tsx` following the existing pattern in other dashboard pages.

---

### Component 6: Dashboard Summary Card â€” `app/dashboard/_components/BotActivityCard.tsx`

**Compact card** for the main dashboard page (`app/dashboard/page.tsx`), similar to the Quick Stat cards.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– AI Bot Activity (30d)               â”‚
â”‚                                          â”‚
â”‚  127 visits Â· 5 active Â· 3 blind spots  â”‚
â”‚  [View Bot Activity â†’]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- If `totalVisits === 0` and no published menu: "Publish your Magic Menu to start tracking AI bots."
- If `totalVisits === 0` but menu published: "No bot visits yet â€” check back in a few days."
- If `blindSpotCount > 0`: highlight in amber/crimson.
- Link to `/dashboard/crawler-analytics`.

Add this card to `app/dashboard/page.tsx` in the Quick Stats section.

---

### Component 7: Migration â€” `supabase/migrations/20260227000002_crawler_hits_location_id.sql`

The current `crawler_hits` table has `menu_id` but no `location_id`. For the dashboard to filter by location (important for Agency tier multi-location orgs), add an optional `location_id` column:

```sql
-- Sprint 73: Add location_id to crawler_hits for multi-location filtering
ALTER TABLE public.crawler_hits
  ADD COLUMN IF NOT EXISTS location_id uuid
  REFERENCES public.locations(id) ON DELETE CASCADE;

-- Backfill existing rows (if any) by joining through magic_menus
UPDATE public.crawler_hits ch
SET location_id = mm.location_id
FROM public.magic_menus mm
WHERE ch.menu_id = mm.id
  AND ch.location_id IS NULL;

-- Index for dashboard queries
CREATE INDEX IF NOT EXISTS idx_crawler_hits_org_location
  ON public.crawler_hits (org_id, location_id, crawled_at DESC);
```

**Update `database.types.ts`:** Add `location_id: string | null` to `crawler_hits` Row/Insert/Update types. Add the FK Relationship.

**Update `prod_schema.sql`:** Add `location_id` column to the `crawler_hits` CREATE TABLE definition.

**Update the internal route** to also write `location_id` when inserting:

```typescript
const { data: menu } = await supabase
  .from('magic_menus')
  .select('id, org_id, location_id')  // â† also fetch location_id
  .eq('public_slug', slug)
  .eq('is_published', true)
  .maybeSingle();

await supabase.from('crawler_hits').insert({
  org_id: menu.org_id,
  menu_id: menu.id,
  location_id: menu.location_id,  // â† write through from magic_menus
  bot_type: botType,
  user_agent: userAgent ?? null,
});
```

---

### Component 8: Seed Data â€” `supabase/seed.sql`

Add seed rows for `crawler_hits` so the dashboard has data in local development:

```sql
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- Section 18: Crawler Hits (Sprint 73)
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- UUIDs: e0eebc99-...-a11 through e5eebc99-...-a11
-- Uses: org_id a0eebc99 (Charcoal N Chill), menu_id from Section 4 (magic_menus)

INSERT INTO public.crawler_hits (id, org_id, menu_id, location_id, bot_type, user_agent, crawled_at) VALUES
  ('e0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'gptbot', 'Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)', NOW() - INTERVAL '2 days'),
  ('e1eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'gptbot', 'Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0)', NOW() - INTERVAL '5 days'),
  ('e2eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'claudebot', 'ClaudeBot/1.0', NOW() - INTERVAL '3 days'),
  ('e3eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'google-extended', 'Mozilla/5.0 (compatible; Google-Extended)', NOW() - INTERVAL '1 day'),
  ('e4eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'google-extended', 'Mozilla/5.0 (compatible; Google-Extended)', NOW() - INTERVAL '7 days'),
  ('e5eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11', '<MENU_UUID>', '<LOCATION_UUID>', 'oai-searchbot', 'OAI-SearchBot/1.0', NOW() - INTERVAL '4 days');
```

**IMPORTANT:** Replace `<MENU_UUID>` and `<LOCATION_UUID>` with the actual UUIDs from the existing seed file's magic_menus and locations sections. Read `supabase/seed.sql` UUID reference card first. Register the new `e0`â€“`e5` UUIDs in the reference card (AI_RULES Â§7 â€” hex only).

---

### Component 9: Golden Tenant Fixture â€” `src/__fixtures__/golden-tenant.ts`

Add `MOCK_CRAWLER_HIT` and `MOCK_CRAWLER_SUMMARY` fixtures:

```typescript
/**
 * Sprint 73 â€” Canonical crawler_hits fixture for Charcoal N Chill.
 * UUIDs match supabase/seed.sql Section 18.
 */
export const MOCK_CRAWLER_HIT = {
  id: 'e0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11',
  org_id: 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11',
  menu_id: '<MENU_UUID>',  // â† replace with actual menu UUID from seed
  bot_type: 'gptbot',
  user_agent: 'Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0)',
  crawled_at: '2026-02-25T12:00:00.000Z',
} as const;

/**
 * Sprint 73 â€” Canonical CrawlerSummary fixture for dashboard tests.
 */
export const MOCK_CRAWLER_SUMMARY: import('@/lib/data/crawler-analytics').CrawlerSummary = {
  totalVisits: 6,
  blindSpotCount: 4,
  bots: [
    { botType: 'google-extended', label: 'Google-Extended', engine: 'Gemini', description: 'Gemini AI training', visitCount: 2, lastVisitAt: '2026-02-26T12:00:00.000Z', status: 'low' },
    { botType: 'gptbot', label: 'GPTBot', engine: 'ChatGPT', description: 'OpenAI training crawler', visitCount: 2, lastVisitAt: '2026-02-25T12:00:00.000Z', status: 'low' },
    { botType: 'oai-searchbot', label: 'OAI-SearchBot', engine: 'ChatGPT Search', description: 'ChatGPT live search', visitCount: 1, lastVisitAt: '2026-02-23T12:00:00.000Z', status: 'low' },
    { botType: 'claudebot', label: 'ClaudeBot', engine: 'Claude', description: 'Anthropic training crawler', visitCount: 1, lastVisitAt: '2026-02-24T12:00:00.000Z', status: 'low' },
  ],
  blindSpots: [
    { botType: 'perplexitybot', label: 'PerplexityBot', engine: 'Perplexity', fixRecommendation: 'Allow PerplexityBot in robots.txt.' },
    { botType: 'meta-external', label: 'Meta-External', engine: 'Meta AI', fixRecommendation: 'Allow meta-externalagent in robots.txt.' },
    { botType: 'bytespider', label: 'Bytespider', engine: 'TikTok/ByteDance', fixRecommendation: 'Ensure your TikTok profile links to your menu page.' },
    { botType: 'amazonbot', label: 'Amazonbot', engine: 'Amazon AI', fixRecommendation: 'Allow Amazonbot in robots.txt.' },
  ],
};
```

---

## ğŸ§ª Testing â€” Write Tests FIRST (AI_RULES Â§4)

### Test File 1: `src/__tests__/unit/bot-detector.test.ts`

**Target: `lib/crawler/bot-detector.ts`**

```
describe('detectAIBot')
  1.  detects GPTBot from full user-agent string
  2.  detects OAI-SearchBot
  3.  detects ChatGPT-User
  4.  detects ClaudeBot
  5.  detects Google-Extended
  6.  detects PerplexityBot
  7.  detects meta-externalagent (case-insensitive)
  8.  detects Bytespider
  9.  detects Amazonbot
  10. detects Applebot-Extended
  11. returns null for regular browser UA (Chrome)
  12. returns null for regular browser UA (Safari)
  13. returns null for Googlebot (NOT Google-Extended â€” regular search bot, not AI)
  14. returns null for empty string
  15. returns null for null
  16. returns null for undefined
  17. returns correct botType, label, engine, description for each match
  18. first match wins when UA contains multiple bot patterns

describe('getAllTrackedBots')
  19. returns all 10 tracked bots
  20. each entry has botType, label, engine, description
```

**20 tests total. No mocks needed â€” pure function.**

### Test File 2: `src/__tests__/unit/crawler-log-route.test.ts`

**Target: `app/api/internal/crawler-log/route.ts`**

```
describe('POST /api/internal/crawler-log')
  1. returns 401 when x-internal-secret is missing
  2. returns 401 when x-internal-secret is wrong
  3. returns 400 when botType is missing
  4. returns 400 when slug is missing
  5. returns { ok: true, logged: false } when no published menu matches slug
  6. returns { ok: true, logged: true } on successful insert
  7. inserts correct org_id, menu_id, location_id, bot_type, user_agent
  8. returns 500 when Supabase INSERT fails
```

**8 tests total.**

**Mock requirements:**
```typescript
vi.mock('@/lib/supabase/server', () => ({
  createServiceRoleClient: vi.fn(),
}));
```
Build mock Supabase client handling `.from('magic_menus').select().eq().eq().maybeSingle()` and `.from('crawler_hits').insert()`. Use `as unknown as SupabaseClient<Database>` (Â§38.2).

Set `process.env.CRON_SECRET = 'test-secret'` in `beforeEach`.

### Test File 3: `src/__tests__/unit/crawler-analytics-data.test.ts`

**Target: `lib/data/crawler-analytics.ts`**

```
describe('fetchCrawlerAnalytics')
  1. returns all 10 bots even when crawler_hits is empty (all blind spots)
  2. aggregates visit counts by bot_type correctly
  3. finds max crawled_at per bot_type for lastVisitAt
  4. marks bots with 0 visits as blind_spot status
  5. marks bots with 1-4 visits as low status
  6. marks bots with 5+ visits as active status
  7. sorts bots by visitCount descending
  8. returns correct blindSpotCount
  9. returns correct totalVisits
  10. includes fix recommendations for each blind spot
  11. filters to last 30 days only (older hits excluded)
  12. scopes query by org_id (belt-and-suspenders)
```

**12 tests total.**

**Mock requirements:** Mock Supabase client returning controlled `crawler_hits` rows.

### Test File 4: `src/__tests__/unit/sidebar-crawler.test.ts`

**Target: `components/layout/Sidebar.tsx` â€” verify new nav entry**

```
describe('Sidebar NAV_ITEMS â€” Bot Activity')
  1. NAV_ITEMS includes Bot Activity entry
  2. Bot Activity has correct href /dashboard/crawler-analytics
  3. Bot Activity is positioned after Page Audits
```

**3 tests total.**

---

## ğŸ“‚ Files to Create/Modify

| # | File | Action | Purpose |
|---|------|--------|---------|
| 1 | `lib/crawler/bot-detector.ts` | **CREATE** | Pure bot detection + registry |
| 2 | `proxy.ts` | **MODIFY** | Add bot detection + fire-and-forget log |
| 3 | `app/api/internal/crawler-log/route.ts` | **CREATE** | Internal route â€” service-role INSERT into crawler_hits |
| 4 | `lib/data/crawler-analytics.ts` | **CREATE** | Data fetcher â€” aggregate crawler_hits + blind spot detection |
| 5 | `app/dashboard/crawler-analytics/page.tsx` | **CREATE** | Bot Activity dashboard page (Server Component) |
| 6 | `app/dashboard/crawler-analytics/error.tsx` | **CREATE** | Error boundary |
| 7 | `app/dashboard/_components/BotActivityCard.tsx` | **CREATE** | Summary card for main dashboard |
| 8 | `app/dashboard/page.tsx` | **MODIFY** | Add BotActivityCard |
| 9 | `lib/data/dashboard.ts` | **MODIFY** | Add crawler summary to DashboardData |
| 10 | `components/layout/Sidebar.tsx` | **MODIFY** | Add Bot Activity to NAV_ITEMS |
| 11 | `supabase/migrations/20260227000002_crawler_hits_location_id.sql` | **CREATE** | Add location_id column + index |
| 12 | `supabase/prod_schema.sql` | **MODIFY** | Add location_id to crawler_hits CREATE TABLE |
| 13 | `lib/supabase/database.types.ts` | **MODIFY** | Add location_id to crawler_hits types |
| 14 | `supabase/seed.sql` | **MODIFY** | Add crawler_hits seed rows |
| 15 | `src/__fixtures__/golden-tenant.ts` | **MODIFY** | Add MOCK_CRAWLER_HIT + MOCK_CRAWLER_SUMMARY |
| 16 | `src/__tests__/unit/bot-detector.test.ts` | **CREATE** | 20 tests â€” pure function |
| 17 | `src/__tests__/unit/crawler-log-route.test.ts` | **CREATE** | 8 tests â€” internal route |
| 18 | `src/__tests__/unit/crawler-analytics-data.test.ts` | **CREATE** | 12 tests â€” data layer |
| 19 | `src/__tests__/unit/sidebar-crawler.test.ts` | **CREATE** | 3 tests â€” sidebar nav |

**Expected test count: 43 new tests across 4 files.**

---

## ğŸš« What NOT to Do

1. **DO NOT edit `middleware.ts`** (AI_RULES Â§6). All middleware logic goes in `proxy.ts`.
2. **DO NOT use `createServiceRoleClient()` inside proxy.ts** â€” Edge middleware can't use the full Supabase SDK. Use the fire-and-forget fetch pattern instead.
3. **DO NOT `await` the fire-and-forget fetch** â€” bot logging must NEVER block page delivery (AI_RULES Â§17).
4. **DO NOT create files under `supabase/functions/`** â€” no Supabase Edge Functions (AI_RULES Â§6).
5. **DO NOT call any AI/LLM API.** Bot detection is string matching, not AI classification.
6. **DO NOT plan-gate Bot Activity** â€” available to all tiers (even trial). It's the wow that drives upgrades.
7. **DO NOT use `as any` on Supabase clients** (AI_RULES Â§38.2).
8. **DO NOT use dynamic Tailwind class construction** for status colors (AI_RULES Â§12). Use literal classes.
9. **DO NOT hardcode fake visit counts** (AI_RULES Â§20). Null data â†’ null state UI.
10. **DO NOT block on the menu slug lookup in middleware** â€” the entire bot logging chain is fire-and-forget.

---

## âœ… Definition of Done (AI_RULES Â§13.5)

- [ ] `lib/crawler/bot-detector.ts` â€” Pure function, 10 bots in registry, case-insensitive matching
- [ ] `proxy.ts` â€” Bot detection wired into menu subdomain handler, fire-and-forget with `.catch(() => {})`
- [ ] `app/api/internal/crawler-log/route.ts` â€” Service-role INSERT, auth via `x-internal-secret`
- [ ] `lib/data/crawler-analytics.ts` â€” Aggregation + blind spot detection + fix recommendations
- [ ] `app/dashboard/crawler-analytics/page.tsx` â€” Full page with bot list, blind spots, null state
- [ ] `app/dashboard/_components/BotActivityCard.tsx` â€” Summary card on main dashboard
- [ ] Sidebar has "Bot Activity" nav entry after Page Audits
- [ ] Migration `20260227000002` adds `location_id` to `crawler_hits`
- [ ] `prod_schema.sql` and `database.types.ts` updated
- [ ] `seed.sql` has 6 crawler_hits rows, UUIDs registered in reference card
- [ ] Golden Tenant fixtures: `MOCK_CRAWLER_HIT`, `MOCK_CRAWLER_SUMMARY`
- [ ] `npx vitest run src/__tests__/unit/bot-detector.test.ts` â€” 20 tests passing
- [ ] `npx vitest run src/__tests__/unit/crawler-log-route.test.ts` â€” 8 tests passing
- [ ] `npx vitest run src/__tests__/unit/crawler-analytics-data.test.ts` â€” 12 tests passing
- [ ] `npx vitest run src/__tests__/unit/sidebar-crawler.test.ts` â€” 3 tests passing
- [ ] `npx vitest run` â€” ALL tests passing, no regressions
- [ ] `npx tsc --noEmit` â€” 0 new type errors
- [ ] DEVLOG.md entry written

---

## ğŸ““ DEVLOG Entry Format (AI_RULES Â§13.2)

```markdown
## 2026-02-27 â€” Sprint 73: AI Crawler Analytics â€” Wire crawler_hits in Middleware (Completed)

**Goal:** Wire the existing but empty `crawler_hits` table to the proxy middleware so AI bot visits to Magic Menu pages are detected and logged, then build a Bot Activity dashboard with blind spot detection and fix recommendations.

**Scope:**
- `lib/crawler/bot-detector.ts` â€” **NEW.** Pure bot detection utility. 10 AI bot user-agents in registry (GPTBot, OAI-SearchBot, ChatGPT-User, ClaudeBot, Google-Extended, PerplexityBot, Meta-External, Bytespider, Amazonbot, Applebot-Extended). Case-insensitive substring matching. Exports: `detectAIBot()`, `getAllTrackedBots()`, `AI_BOT_REGISTRY`.
- `proxy.ts` â€” **MODIFIED.** Added bot detection in menu subdomain handler. Fire-and-forget `fetch()` to `/api/internal/crawler-log` with `x-internal-secret` header. Never awaited â€” bot logging cannot block page delivery. `.catch(() => {})` absorbs errors (Â§17).
- `app/api/internal/crawler-log/route.ts` â€” **NEW.** Internal POST endpoint. Auth via `x-internal-secret` matching `CRON_SECRET`. Looks up magic_menu by `public_slug`, INSERTs into `crawler_hits` via `createServiceRoleClient()`. Returns `{ ok, logged }`.
- `lib/data/crawler-analytics.ts` â€” **NEW.** Data fetcher. Aggregates `crawler_hits` last 30 days by bot_type. Cross-references with AI_BOT_REGISTRY for blind spot detection. Status thresholds: â‰¥5=active, 1-4=low, 0=blind_spot. Fix recommendations per engine.
- `app/dashboard/crawler-analytics/page.tsx` â€” **NEW.** Server Component. Summary strip (total visits, active bots, blind spots), per-bot activity list sorted by count, blind spot section with fix recommendations, null state for new tenants.
- `app/dashboard/crawler-analytics/error.tsx` â€” **NEW.** Error boundary.
- `app/dashboard/_components/BotActivityCard.tsx` â€” **NEW.** Summary card for main dashboard with visit count, active/blind spot counts, link to full page.
- `app/dashboard/page.tsx` â€” **MODIFIED.** Added BotActivityCard to Quick Stats section.
- `components/layout/Sidebar.tsx` â€” **MODIFIED.** Added "Bot Activity" to NAV_ITEMS with Bot icon, between Page Audits and AI Assistant.
- `supabase/migrations/20260227000002_crawler_hits_location_id.sql` â€” **NEW.** Adds `location_id` column to `crawler_hits` with FK to locations, backfill from magic_menus, composite index.
- `supabase/prod_schema.sql` â€” **MODIFIED.** Added `location_id` to crawler_hits.
- `lib/supabase/database.types.ts` â€” **MODIFIED.** Added `location_id` to crawler_hits types + FK relationship.
- `supabase/seed.sql` â€” **MODIFIED.** Added 6 crawler_hits seed rows (UUIDs e0â€“e5). Updated UUID reference card.
- `src/__fixtures__/golden-tenant.ts` â€” **MODIFIED.** Added MOCK_CRAWLER_HIT, MOCK_CRAWLER_SUMMARY.

**Tests added:**
- `src/__tests__/unit/bot-detector.test.ts` â€” **N Vitest tests.** All 10 bots detected, browser UAs rejected, null/empty/undefined handled, getAllTrackedBots returns full registry.
- `src/__tests__/unit/crawler-log-route.test.ts` â€” **N Vitest tests.** Auth guard, missing fields, no-menu-found, successful INSERT, Supabase error handling.
- `src/__tests__/unit/crawler-analytics-data.test.ts` â€” **N Vitest tests.** Aggregation, blind spot detection, status thresholds, 30-day filtering, fix recommendations.
- `src/__tests__/unit/sidebar-crawler.test.ts` â€” **N Vitest tests.** NAV_ITEMS includes Bot Activity with correct href and position.

**Run commands:**
```bash
npx vitest run src/__tests__/unit/bot-detector.test.ts             # N tests passing
npx vitest run src/__tests__/unit/crawler-log-route.test.ts        # N tests passing
npx vitest run src/__tests__/unit/crawler-analytics-data.test.ts   # N tests passing
npx vitest run src/__tests__/unit/sidebar-crawler.test.ts          # N tests passing
npx vitest run                                                      # All tests passing
```

**Note:** Replace N with actual test counts verified via `grep -cE "^\s*(it|test)\("` (AI_RULES Â§13.3).
```

---

## ğŸ”— Sprint Dependencies

| Dependency | Sprint | What It Provides |
|-----------|--------|-----------------|
| `crawler_hits` table in schema | Initial schema | Table exists, RLS policies exist, FK to magic_menus |
| `proxy.ts` middleware | Sprint 62 (Â§37.3) | Subdomain routing for menu.* â†’ `/m/` rewrite |
| `magic_menus` with `public_slug` | Sprint 38-40 | Slug-based menu lookup for bot logging |
| Sidebar NAV_ITEMS pattern | Sprint 68 | Exported NAV_ITEMS array for testability |
| Dashboard data layer | Sprint 64 | `lib/data/` pattern for data fetchers |
| Golden Tenant fixtures | All sprints | Canonical test data |

---

## ğŸ§  Edge Cases to Handle

1. **Bot visits before any menu published:** The internal route returns `{ logged: false }` â€” no INSERT. The middleware `.catch()` handles this gracefully.
2. **Same bot visits multiple times per day:** Each visit is a separate row. Aggregation happens at query time. No deduplication needed â€” raw data is more valuable.
3. **UA string contains multiple bot patterns:** First match wins. Order in `AI_BOT_REGISTRY` matters.
4. **Non-AI Googlebot:** `Googlebot` (without `-Extended`) is NOT an AI crawler â€” it's the regular search bot. Must NOT be detected as a match. Test this explicitly.
5. **CRON_SECRET not set in dev:** The internal route returns 401 for all bot log requests. This is acceptable in local dev â€” crawler analytics is a production feature.
6. **High-volume bot crawling:** The fire-and-forget pattern means the middleware never slows down. The internal route does one INSERT per request. For very high volumes, a future sprint could batch inserts.
7. **Rival tenant cannot see Charcoal N Chill's crawler data:** RLS `org_isolation_select` policy ensures isolation. Verify with existing RLS test patterns.

---

## ğŸ”® AI_RULES Update (if needed)

If this sprint introduces a new engineering constraint, add it to `AI_RULES.md`. Likely candidate:

```markdown
## 40. ğŸ¤– AI Bot Detection Registry â€” Centralized in `lib/crawler/bot-detector.ts` (Sprint 73)

All AI bot user-agent detection is centralized in `lib/crawler/bot-detector.ts`. The `AI_BOT_REGISTRY` array is the single source of truth for known AI crawlers.

* **Rule:** Never hardcode bot UA patterns inline in middleware, routes, or services. Always import from `bot-detector.ts`.
* **Adding new bots:** Append to `AI_BOT_REGISTRY`. The order matters â€” first match wins.
* **Dashboard display:** `getAllTrackedBots()` returns the full registry including bots with 0 visits. New bots added to the registry automatically appear in the dashboard.
```
